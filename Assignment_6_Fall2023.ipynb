{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smitithaparDartmouth/ENGS-108-Fall-23-assignment_2_Fall2023--Smiti-Thapar-/blob/main/Assignment_6_Fall2023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd0qJjGWPDEY"
      },
      "source": [
        "# **ENGS 108 Fall 2023 Assignment 6**\n",
        "\n",
        "*Due November 14, 2023 at 11:59PM on Github*\n",
        "\n",
        "**Instructors:** George Cybenko\n",
        "\n",
        "**TAs:** Sunishka Jain, Diksha Bubna, Ganesh Rohit Nirogi , Kushagra Rawat\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## **Rules and Requirements**\n",
        "\n",
        "\n",
        "1.   You are only allowed to use Python packages that are explicity imported in\n",
        "the assignment notebook or are standard (bultin) python libraries like random, os, sys, etc, (Standard Bultin Python libraries will have a Python.org documentation). For this assignment you may use:\n",
        "  *   [numpy](https://numpy.org/doc/stable/)\n",
        "  *   [pandas](https://pandas.pydata.org/pandas-docs/stable/index.html)\n",
        "  *   [scikit-learn](https://scikit-learn.org/stable/)\n",
        "  *   [matplotlib](https://matplotlib.org/)\n",
        "  *   [tensorflow](https://www.tensorflow.org/)\n",
        "\n",
        "2.   All code must be fit into the designated code or text blocks in the assignment notebook. They are indentified by a **TODO** qualifier.\n",
        "\n",
        "3. For analytical questions that don't require code, type your answer cleanly in Markdown. For help, see the [Google Colab Markdown Guide](https://colab.research.google.com/notebooks/markdown_guide.ipynb).\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' Import Statements '''\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import tqdm\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "jWeL44Q9isdH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abf57fd1-4b00-488a-d3e0-364e1f5f25e9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nrT0KUOtZE1"
      },
      "source": [
        "# Creating a Trump Tweet Generator!!!\n",
        "Finally it's happened. I am allowed to make my dream assignment. Trust me it's the best assignment. People tell me all the time how they wish they could have had an assignment as great as this. You will see why! But first some housekeeping..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFcqjf9VXHfF"
      },
      "source": [
        "# Data Loading\n",
        "In this assignment we will be using a repository of Donald Trump tweets scrapped from Twitter through June 2020 from [Kaggle](https://www.kaggle.com/datasets/austinreese/trump-tweets) and will use the following code blocks to download the dataset directly to your Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tZ4gJ5zuZjP"
      },
      "source": [
        "## Creating a Kaggle API Token\n",
        "First we will need to download an API token from Kaggle in order to download the dataset, so our first step is to create a Kaggle account if you don't already have one. (You should have done this in Assignment 4 in case this looks familiar.\n",
        "1. Create a Kaggle account by following the sign up instructions [here](https://www.kaggle.com/).\n",
        "2. Log into your Kaggle account and click your account icon on the upper righthand side.\n",
        "3. Then select **Account** from the dropdown/sidebar menu.\n",
        "4. Scroll down to the **API** section and select **Create New API Token**.\n",
        "5. This will download a JSON file called kaggle.json to your Downloads folder on your computer.\n",
        "6. Now run the following code block and when the **Browse** button appears, click it and select that kaggle.json file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gSQSg2HkXaHF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "f3743f9c-cc0e-46cc-f826-91802b9f53ff"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4b67f02b-613a-48b5-a014-27e21e05a708\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4b67f02b-613a-48b5-a014-27e21e05a708\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "mkdir: cannot create directory ‘/content/result’: File exists\n"
          ]
        }
      ],
      "source": [
        "# Run this code block after creating a Kaggle API token as instructed above.\n",
        "! pip install -q kaggle\n",
        "from google.colab import files\n",
        "\n",
        "# Will ask you to upload kaggle.json file and remove any old ones.\n",
        "if os.path.exists('kaggle.json'):\n",
        "  os.remove('kaggle.json')\n",
        "files.upload()\n",
        "\n",
        "# Will create the appropriate directory structure\n",
        "if not os.path.exists('/root/.kaggle'):\n",
        "  ! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "# Also we are going to make a directory called result\n",
        "if not os.path.exists('/content/results'):\n",
        "  ! mkdir /content/result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVOhg0Rxx1HB"
      },
      "source": [
        "## Downloading the Dataset\n",
        "\n",
        "7. Now we have downloaded our Kaggle credentials we can now download the Trump Tweets Dataset (or any other Kaggle dataset for that matter) directly into our Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-r2Nya5yV3hW",
        "outputId": "1ce1142f-0e5e-4f25-c20e-c2a7d9ba2cd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trump-tweets.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  trump-tweets.zip\n",
            "replace realdonaldtrump.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: realdonaldtrump.csv     \n",
            "replace trumptweets.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: trumptweets.csv         \n"
          ]
        }
      ],
      "source": [
        "! kaggle datasets download austinreese/trump-tweets\n",
        "# Will check to see if the yoga postures zip file has been unzipped and will unzip the file if not.\n",
        "! unzip trump-tweets.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyYOK_sA1rxB"
      },
      "source": [
        "## Loading the Dataset using Pandas\n",
        "Now let's inspect the trump tweets dataset and see what we have to work with... Brace yourselves."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's load in the two files that we inflated from the Kaggle download. Both realdonaldtrump.csv and trumptweets.csv are the same.\n",
        "real_donald_trump_df = pd.read_csv('realdonaldtrump.csv')"
      ],
      "metadata": {
        "id": "MsrzvV3aizac"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real_donald_trump_df.head()"
      ],
      "metadata": {
        "id": "1Api_2Vbi2Iq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "1ceb84f7-ddc4-4506-fe1b-39d83c9ef523"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           id                                               link  \\\n",
              "0  1698308935  https://twitter.com/realDonaldTrump/status/169...   \n",
              "1  1701461182  https://twitter.com/realDonaldTrump/status/170...   \n",
              "2  1737479987  https://twitter.com/realDonaldTrump/status/173...   \n",
              "3  1741160716  https://twitter.com/realDonaldTrump/status/174...   \n",
              "4  1773561338  https://twitter.com/realDonaldTrump/status/177...   \n",
              "\n",
              "                                             content                 date  \\\n",
              "0  Be sure to tune in and watch Donald Trump on L...  2009-05-04 13:54:25   \n",
              "1  Donald Trump will be appearing on The View tom...  2009-05-04 20:00:10   \n",
              "2  Donald Trump reads Top Ten Financial Tips on L...  2009-05-08 08:38:08   \n",
              "3  New Blog Post: Celebrity Apprentice Finale and...  2009-05-08 15:40:15   \n",
              "4  \"My persona will never be that of a wallflower...  2009-05-12 09:07:28   \n",
              "\n",
              "   retweets  favorites mentions hashtags  \n",
              "0       510        917      NaN      NaN  \n",
              "1        34        267      NaN      NaN  \n",
              "2        13         19      NaN      NaN  \n",
              "3        11         26      NaN      NaN  \n",
              "4      1375       1945      NaN      NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-673e99b7-0b95-494d-960d-b286671053a9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>link</th>\n",
              "      <th>content</th>\n",
              "      <th>date</th>\n",
              "      <th>retweets</th>\n",
              "      <th>favorites</th>\n",
              "      <th>mentions</th>\n",
              "      <th>hashtags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1698308935</td>\n",
              "      <td>https://twitter.com/realDonaldTrump/status/169...</td>\n",
              "      <td>Be sure to tune in and watch Donald Trump on L...</td>\n",
              "      <td>2009-05-04 13:54:25</td>\n",
              "      <td>510</td>\n",
              "      <td>917</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1701461182</td>\n",
              "      <td>https://twitter.com/realDonaldTrump/status/170...</td>\n",
              "      <td>Donald Trump will be appearing on The View tom...</td>\n",
              "      <td>2009-05-04 20:00:10</td>\n",
              "      <td>34</td>\n",
              "      <td>267</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1737479987</td>\n",
              "      <td>https://twitter.com/realDonaldTrump/status/173...</td>\n",
              "      <td>Donald Trump reads Top Ten Financial Tips on L...</td>\n",
              "      <td>2009-05-08 08:38:08</td>\n",
              "      <td>13</td>\n",
              "      <td>19</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1741160716</td>\n",
              "      <td>https://twitter.com/realDonaldTrump/status/174...</td>\n",
              "      <td>New Blog Post: Celebrity Apprentice Finale and...</td>\n",
              "      <td>2009-05-08 15:40:15</td>\n",
              "      <td>11</td>\n",
              "      <td>26</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1773561338</td>\n",
              "      <td>https://twitter.com/realDonaldTrump/status/177...</td>\n",
              "      <td>\"My persona will never be that of a wallflower...</td>\n",
              "      <td>2009-05-12 09:07:28</td>\n",
              "      <td>1375</td>\n",
              "      <td>1945</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-673e99b7-0b95-494d-960d-b286671053a9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-673e99b7-0b95-494d-960d-b286671053a9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-673e99b7-0b95-494d-960d-b286671053a9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-bce8750c-d13d-4420-8443-cd9ee7359501\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bce8750c-d13d-4420-8443-cd9ee7359501')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-bce8750c-d13d-4420-8443-cd9ee7359501 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mty9AB1bd5hH"
      },
      "source": [
        "## Problem 1: Pre-Processing the Tweets\n",
        "As you may have noticed from the dataframe we just loaded, there are some special characters we need to handle. If we are making a tweet or sentence generator, we don't want to mess with special characters like commas or colons or really even captialization. So in the following section you are going to preprocess the data and strip these special characters out."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EJAqYcB3VyQm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8V1hjp0rhOw"
      },
      "source": [
        "### Task 1: The Preprocess function\n",
        "In the following code block complete the preprocess function that will strip or substitute out various special characters or sequences of characters that may not be ideal when training a sentence generator. We will use the built-in re python library to do a number of substitutions, and I have given you a skeleton below, see if you can complete it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IX8v3DzArOf6"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "REGEX_SUBS = {\n",
        "  '\\\\\\\\': ' ',  # Use '\\\\\\\\' instead of '\\\\'\n",
        "  '\\n': ' ',\n",
        "  '&': '',\n",
        "  'RT ': '',\n",
        "  '~': '',\n",
        "  '#': '',\n",
        "  '!+': '',\n",
        "  r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)': 'link',  # Use raw string (prefix 'r') here\n",
        "  r'\\\\\\'': '',  # Corrected line\n",
        "  r'[@]\\w+': 'user',  # Use raw string (prefix 'r') here\n",
        "  '[:|;]': '',\n",
        "  r'[\\\\x]\\W+|\\d': '',  # Use raw string (prefix 'r') here\n",
        "  r'[\\\\x]\\w+': '',  # Use raw string (prefix 'r') here\n",
        "  '    ': ' ',\n",
        "  '   ': ' ',\n",
        "  '  ': ' ',\n",
        "  '@': ' '\n",
        "}\n",
        "\n",
        "def preprocess(text):\n",
        "    for key, value in REGEX_SUBS.items():\n",
        "        text = re.sub(key, value, text)\n",
        "    text = text.lower()  # Make the text lowercase\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_Ln5qixfHwt"
      },
      "source": [
        "### Task 2: Preprocess the dataset.\n",
        "Now that we have our preprocess function, let's preprocess all the tweets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZCY6MmC-6HhX"
      },
      "outputs": [],
      "source": [
        "#TODO: Run all the Dataframe content through your preprocess function.\n",
        "real_donald_trump_df['content'] = real_donald_trump_df['content'].apply(preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "real_donald_trump_df['content']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIoqSFR3Tj3-",
        "outputId": "7aa6121a-5862-4bc1-ac6d-e0020b354ca4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        be sure to tune in and watch donald trump on l...\n",
              "1        donald trump will be appearing on the view tom...\n",
              "2        donald trump reads top ten financial tips on l...\n",
              "3        new blog post celebrity apprentice finale and ...\n",
              "4        \"my persona will never be that of a wallflower...\n",
              "                               ...                        \n",
              "43347    joe biden was a total failure in government. h...\n",
              "43348    will be interviewed on   seanhannity tonight a...\n",
              "43349                                                 link\n",
              "43350                                                 link\n",
              "43351                                                 link\n",
              "Name: content, Length: 43352, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0530JEhj40JU"
      },
      "source": [
        "### Task 3: Tokenize the Data\n",
        "The next step in every Natural Language Processing task is to tokenize the data, i.e., seperate our words, special characters, etc. into separate unique tokens. We will be using the [Natural Language Tool Kit](https://www.nltk.org/index.html) in python to accomplish this. Study the nltk API docs and see if you can tokenize our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "A5jwnhoU5cn7"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "tokenized = [word_tokenize(tweet) for tweet in real_donald_trump_df['content']]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UC58Uu6E3AgX"
      },
      "source": [
        "### Task 4: Build a Vocabulary\n",
        "Now that we have our data tokenized, let's build a vocabulary including beginning and ending of sentence tokens, i.e., \\<s>, \\</s> for example. At this point let's also add in these beginning and end tokens into each of our data instances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Zr7jeaZf0lXX"
      },
      "outputs": [],
      "source": [
        "vocab_keys = {'<s>', '</s>'}\n",
        "# Step 1: Extend vocab_keys with all unique tokens from tokenized data\n",
        "for tweet_tokens in tokenized:\n",
        "    vocab_keys.update(tweet_tokens)\n",
        "\n",
        "# Step 2: Create vocabulary and inverse vocabulary dictionaries\n",
        "# Rebuild the vocabulary to include all tokens\n",
        "vocabulary = {token: idx for idx, token in enumerate(vocab_keys)}\n",
        "vocabulary_inverse = {idx: token for token, idx in vocabulary.items()}\n",
        "\n",
        "# Step 3: Encode training data\n",
        "train_data = []\n",
        "for tweet_tokens in tokenized:\n",
        "    # Adding beginning and end tokens to each tokenized tweet\n",
        "    tweet_tokens = ['<s>'] + tweet_tokens + ['</s>']\n",
        "    # Converting tokens to their corresponding indices\n",
        "    encoded_tweet = [vocabulary[token] for token in tweet_tokens]\n",
        "    train_data.append(encoded_tweet)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ELSFJ8LXQSN",
        "outputId": "cd45cd26-ec5a-47c3-b650-d4c197ed1c31"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'damnit': 0,\n",
              " 'cjtf–oir': 1,\n",
              " 'commit': 2,\n",
              " 'amalfas': 3,\n",
              " 'amilcaralvarez': 4,\n",
              " 'scorned': 5,\n",
              " 'chernuna': 6,\n",
              " 'khan': 7,\n",
              " 'docs': 8,\n",
              " 'lainey': 9,\n",
              " 'abdeslam': 10,\n",
              " 'alfonse': 11,\n",
              " 'also': 12,\n",
              " 'pbrunnerrr': 13,\n",
              " \"'trumping\": 14,\n",
              " 'assemble': 15,\n",
              " 'nydailynews': 16,\n",
              " 'lanas': 17,\n",
              " 'hindenburg': 18,\n",
              " 'अधिक': 19,\n",
              " 'bigpeter': 20,\n",
              " 'best_is_me': 21,\n",
              " 'mavelous': 22,\n",
              " 'demonstrated': 23,\n",
              " 'sirenasola': 24,\n",
              " 'tijfbutler': 25,\n",
              " 'bleed': 26,\n",
              " 'kiernanproud': 27,\n",
              " 'thai': 28,\n",
              " 'helsinki': 29,\n",
              " 'watered': 30,\n",
              " 'bizpacreview': 31,\n",
              " 'colle…': 32,\n",
              " 'world-famous': 33,\n",
              " 'judgement': 34,\n",
              " 'daily_record': 35,\n",
              " 'markwcarlson': 36,\n",
              " 'tcloer': 37,\n",
              " 'planes': 38,\n",
              " 'swampman': 39,\n",
              " 'erictrump-': 40,\n",
              " 'celebrityapprentice': 41,\n",
              " 'fpatterson': 42,\n",
              " 'rohrabacher': 43,\n",
              " 'spazosaurus': 44,\n",
              " 'aleady': 45,\n",
              " 'brain': 46,\n",
              " 'd-calif': 47,\n",
              " 'bike': 48,\n",
              " 'wikileaks': 49,\n",
              " 'floyd': 50,\n",
              " 'oversplayer': 51,\n",
              " 'minute': 52,\n",
              " 'folk': 53,\n",
              " 'c_archaeology': 54,\n",
              " 'touch-': 55,\n",
              " 'michaelpleahy': 56,\n",
              " 'courageous': 57,\n",
              " 'video-link': 58,\n",
              " 'chateauemissary': 59,\n",
              " 'volunteers': 60,\n",
              " 'aims': 61,\n",
              " 'northcutt': 62,\n",
              " 'replayed': 63,\n",
              " 'whywinblo': 64,\n",
              " 'harbor.': 65,\n",
              " 'obstructionlink': 66,\n",
              " 'challenge': 67,\n",
              " 'clewdonaldtrump': 68,\n",
              " '-fantastic': 69,\n",
              " 'rearing': 70,\n",
              " 'delighted': 71,\n",
              " 'respected—': 72,\n",
              " 'newscomauhq': 73,\n",
              " 'jonnati': 74,\n",
              " 'catastrophic': 75,\n",
              " 'turnberry-': 76,\n",
              " 'fdn': 77,\n",
              " 'gplunkert': 78,\n",
              " 'murrayjames': 79,\n",
              " 'theresa_cali': 80,\n",
              " 'jews': 81,\n",
              " 'ill-fit': 82,\n",
              " 'wingchungun': 83,\n",
              " 'fifth': 84,\n",
              " 'cfpb': 85,\n",
              " 'yeacountry': 86,\n",
              " 'anti-police': 87,\n",
              " 'vanguard': 88,\n",
              " 'harrislink': 89,\n",
              " 'realmarkzuck': 90,\n",
              " 'votetrumpmaga': 91,\n",
              " 'choosing': 92,\n",
              " 'misrepresenting': 93,\n",
              " 'ignatiusgreilly': 94,\n",
              " 'anthonycl': 95,\n",
              " 'schapiro': 96,\n",
              " 'modernizing': 97,\n",
              " 'spit-': 98,\n",
              " 'paranormalfact': 99,\n",
              " 'aguscaloevery': 100,\n",
              " 'bebe_el_mejor': 101,\n",
              " 'philmickelson': 102,\n",
              " 'whitewashing': 103,\n",
              " 'sdallink': 104,\n",
              " 'jeffzeleny': 105,\n",
              " 'cargo': 106,\n",
              " 'individual.': 107,\n",
              " 'trumpfimeganena': 108,\n",
              " 'bigop': 109,\n",
              " 'co-hosted': 110,\n",
              " 'built—i': 111,\n",
              " 'china—like': 112,\n",
              " 'zuker': 113,\n",
              " 'soupman': 114,\n",
              " 'wsjopinion': 115,\n",
              " 'kingtrump': 116,\n",
              " 'whrn': 117,\n",
              " 'mychemicalcarli': 118,\n",
              " 'gonzobeachboy': 119,\n",
              " '—it': 120,\n",
              " 'transportation': 121,\n",
              " 'mccabes': 122,\n",
              " 'hoastarts': 123,\n",
              " 'michaeljackson': 124,\n",
              " 'lowly': 125,\n",
              " 'ivy': 126,\n",
              " 'ksa': 127,\n",
              " 'cincodemayolink': 128,\n",
              " 'soccer': 129,\n",
              " 'cnndebatelink': 130,\n",
              " 'back.-': 131,\n",
              " 'eminence': 132,\n",
              " 'birthplace': 133,\n",
              " 'giveback': 134,\n",
              " 'grant_wyeth': 135,\n",
              " 'enzotorin': 136,\n",
              " 'parker_votes': 137,\n",
              " 'resolution': 138,\n",
              " 'inc.-buffy': 139,\n",
              " 'fi': 140,\n",
              " 'fivestarmagazin': 141,\n",
              " 'wsjatlarge': 142,\n",
              " 'team.': 143,\n",
              " 'danielhr': 144,\n",
              " 'dreams.': 145,\n",
              " 'sleeps': 146,\n",
              " 'wichita': 147,\n",
              " 'gratification': 148,\n",
              " 'risen': 149,\n",
              " 'scott': 150,\n",
              " 'stargazer': 151,\n",
              " 'patton': 152,\n",
              " 'kristy': 153,\n",
              " 'located': 154,\n",
              " 'qassem': 155,\n",
              " 'story.': 156,\n",
              " 'eliotspitzer': 157,\n",
              " 'federation': 158,\n",
              " \"'world\": 159,\n",
              " 'financially/weapons': 160,\n",
              " 'ball': 161,\n",
              " 'fiiibuster': 162,\n",
              " 'resigining': 163,\n",
              " 'transactions': 164,\n",
              " 'relaand': 165,\n",
              " 'particular': 166,\n",
              " 'faithable': 167,\n",
              " 'hoaeven': 168,\n",
              " 'linclonone': 169,\n",
              " 'angiecallahan': 170,\n",
              " '_milfncookies': 171,\n",
              " 'repcummings': 172,\n",
              " 'seanhannitylink': 173,\n",
              " 'cory_wise': 174,\n",
              " 'tasystemlink': 175,\n",
              " 'phadraig': 176,\n",
              " 'firefightersfordonald': 177,\n",
              " 'dana': 178,\n",
              " 'republicans.': 179,\n",
              " 'perjury': 180,\n",
              " 'jsalzinger': 181,\n",
              " 'wand.': 182,\n",
              " 'the_lateshow': 183,\n",
              " 'eowdc': 184,\n",
              " 'endorsementlink': 185,\n",
              " 'cared': 186,\n",
              " 'senshelby': 187,\n",
              " 'boardroom-': 188,\n",
              " 'w/equally': 189,\n",
              " 'sect': 190,\n",
              " 'senategoplink': 191,\n",
              " 'dropuser': 192,\n",
              " 'eppisode': 193,\n",
              " 'iacaucus': 194,\n",
              " 'thesportsswede': 195,\n",
              " 'stuf': 196,\n",
              " 'si': 197,\n",
              " 'unreasonable': 198,\n",
              " 'lined': 199,\n",
              " 'roof': 200,\n",
              " 'myale': 201,\n",
              " 'cobra': 202,\n",
              " 'baracktax': 203,\n",
              " 'grossed': 204,\n",
              " '-acre': 205,\n",
              " 'hltweets': 206,\n",
              " 'eleccion': 207,\n",
              " 'nuts': 208,\n",
              " 'rob_bieber': 209,\n",
              " 'collection': 210,\n",
              " 'batman': 211,\n",
              " 'crew': 212,\n",
              " 'vocabulary': 213,\n",
              " 'trump/cruz': 214,\n",
              " 'mill': 215,\n",
              " 'realmissctusa': 216,\n",
              " 'mops': 217,\n",
              " 'cowpokes': 218,\n",
              " 'trice': 219,\n",
              " 'contact': 220,\n",
              " 'jbeanlll': 221,\n",
              " 'votetrumppence': 222,\n",
              " 'rongreenjr': 223,\n",
              " 'mediabuzz': 224,\n",
              " 'saadvisory': 225,\n",
              " 'tn_riverfolk': 226,\n",
              " 'votetrumpil': 227,\n",
              " 'joanna': 228,\n",
              " 'webisode': 229,\n",
              " 'stifle': 230,\n",
              " 'please': 231,\n",
              " 'nicoleamarin': 232,\n",
              " 'disclosing': 233,\n",
              " 'hard.': 234,\n",
              " 'litigate': 235,\n",
              " 'darlenemichele': 236,\n",
              " 'christophercud': 237,\n",
              " 'shall': 238,\n",
              " 'fortunes': 239,\n",
              " 'blumenthal': 240,\n",
              " 'watershed': 241,\n",
              " 'pro-life': 242,\n",
              " 'possible': 243,\n",
              " 'workwithkg': 244,\n",
              " 'subscription-': 245,\n",
              " 'jimpethokoukis': 246,\n",
              " 'undertake': 247,\n",
              " 'rkdrake': 248,\n",
              " 'jeffpetak': 249,\n",
              " 'slabs': 250,\n",
              " 'heiressarts': 251,\n",
              " 'elegant': 252,\n",
              " 'presssec': 253,\n",
              " 'referees': 254,\n",
              " 'define': 255,\n",
              " 'slinging': 256,\n",
              " 'unnerving': 257,\n",
              " 'outbreak': 258,\n",
              " 'vying': 259,\n",
              " 'marvelous': 260,\n",
              " 'obamacareinthreewords': 261,\n",
              " 'clarkrogers': 262,\n",
              " 'phoenix': 263,\n",
              " 'lastly': 264,\n",
              " \"'to\": 265,\n",
              " 'peachespulliam': 266,\n",
              " 'captures': 267,\n",
              " 'worn': 268,\n",
              " 'interact': 269,\n",
              " 'improvements': 270,\n",
              " 'lorenzolamas': 271,\n",
              " 'ahmadinejad': 272,\n",
              " 'kingvantes': 273,\n",
              " 'atheistwwonka': 274,\n",
              " 'ripe': 275,\n",
              " 'shane': 276,\n",
              " 'govdunleavy': 277,\n",
              " 'likes': 278,\n",
              " 'whistleblowers': 279,\n",
              " 'errol': 280,\n",
              " 'stitchjonze': 281,\n",
              " 'djscotter': 282,\n",
              " 'costing': 283,\n",
              " 'itsemilyshine': 284,\n",
              " 'bandon': 285,\n",
              " 'tony_leers': 286,\n",
              " 'villages-': 287,\n",
              " 'scammer': 288,\n",
              " 'made': 289,\n",
              " 'palmisano': 290,\n",
              " 'ac': 291,\n",
              " 'flinched': 292,\n",
              " 'matthewmercer': 293,\n",
              " 'history-link': 294,\n",
              " 'celebrities': 295,\n",
              " 'courses/': 296,\n",
              " 'nomination-': 297,\n",
              " 'brokerage': 298,\n",
              " 'bailing': 299,\n",
              " 'reopening': 300,\n",
              " 'released': 301,\n",
              " 'tdragonfly': 302,\n",
              " 'advertina': 303,\n",
              " 'youngsellz': 304,\n",
              " 'sandeepsrih': 305,\n",
              " 'shoot-link': 306,\n",
              " 'renovate': 307,\n",
              " 'nunosinaustin': 308,\n",
              " 'interfere': 309,\n",
              " 'hakes': 310,\n",
              " 'psu_rebot': 311,\n",
              " 'flyers': 312,\n",
              " ',just': 313,\n",
              " 'anderson': 314,\n",
              " 'obrador': 315,\n",
              " 'wsjvideo': 316,\n",
              " 'atlasairww': 317,\n",
              " 'pbsgwen': 318,\n",
              " 'newbluemonster': 319,\n",
              " 'vin': 320,\n",
              " 'officer.': 321,\n",
              " 'tiaffo': 322,\n",
              " 'seminar': 323,\n",
              " 'pocket': 324,\n",
              " 'goooo': 325,\n",
              " 'handled': 326,\n",
              " 'makeamericagrateagain': 327,\n",
              " 'nude': 328,\n",
              " 'rock-gets': 329,\n",
              " 'emmanuel': 330,\n",
              " 'survey.': 331,\n",
              " 'rainnwilson': 332,\n",
              " 'the_news_diva': 333,\n",
              " 'bewhatjesuswas': 334,\n",
              " 'craig_longislnd': 335,\n",
              " 'nh': 336,\n",
              " 'tlink': 337,\n",
              " 'joni': 338,\n",
              " 'matthewhawilo': 339,\n",
              " 'gravismarketing': 340,\n",
              " 'yoho': 341,\n",
              " 'teens': 342,\n",
              " '-stuart': 343,\n",
              " 'bahamas': 344,\n",
              " 'pivot': 345,\n",
              " 'pa.': 346,\n",
              " 'drought': 347,\n",
              " 'barbarasmayer': 348,\n",
              " 'pirates': 349,\n",
              " 'indecision.': 350,\n",
              " 'input': 351,\n",
              " 'minions': 352,\n",
              " 'girls': 353,\n",
              " 'quitter': 354,\n",
              " 'applications': 355,\n",
              " 'deranged': 356,\n",
              " 'overturns': 357,\n",
              " 'morning—total': 358,\n",
              " 'based': 359,\n",
              " 'fletcher': 360,\n",
              " 'michaeljohns': 361,\n",
              " 'handwriting': 362,\n",
              " 'uncomfortable': 363,\n",
              " 'gotmilk': 364,\n",
              " 'shahira': 365,\n",
              " '-not': 366,\n",
              " 'fiis': 367,\n",
              " 'carnivalcruise': 368,\n",
              " 'kevinwarchie': 369,\n",
              " 'elonmusk': 370,\n",
              " 'kamp': 371,\n",
              " 'engage': 372,\n",
              " 'quotas': 373,\n",
              " 'twendency': 374,\n",
              " 'bads': 375,\n",
              " 'te.': 376,\n",
              " 'fasttracktalent': 377,\n",
              " 'akmackey': 378,\n",
              " 'technology': 379,\n",
              " 'greenberg': 380,\n",
              " 'pellscuse': 381,\n",
              " 'penalty-but': 382,\n",
              " 'screwup': 383,\n",
              " 'littleleague': 384,\n",
              " 'jepsen': 385,\n",
              " 'chrisrosica': 386,\n",
              " 'kverb': 387,\n",
              " 'gina': 388,\n",
              " 'slide': 389,\n",
              " 'free': 390,\n",
              " 'actoninstitute': 391,\n",
              " 'dancer': 392,\n",
              " 'cop': 393,\n",
              " 'maps': 394,\n",
              " 'qe': 395,\n",
              " 'latest': 396,\n",
              " 'efbi': 397,\n",
              " 'e-mail': 398,\n",
              " 'soleil': 399,\n",
              " 'sheckiezrealdonaldtrump': 400,\n",
              " 'andreatantaros': 401,\n",
              " 'incfw': 402,\n",
              " 'endowment': 403,\n",
              " 'scotland…': 404,\n",
              " 'comparable': 405,\n",
              " 'problem-do': 406,\n",
              " 'flavors': 407,\n",
              " 'theodopolis_': 408,\n",
              " 'grace': 409,\n",
              " 'infighting': 410,\n",
              " 'reduce—are': 411,\n",
              " 'channels': 412,\n",
              " 'disclaimer': 413,\n",
              " 'millennials': 414,\n",
              " 'mcchrystal': 415,\n",
              " 'rossmccutchen': 416,\n",
              " 'jeh': 417,\n",
              " 'cletendre': 418,\n",
              " 'republic.': 419,\n",
              " 'jasonbordui': 420,\n",
              " 'iklemieutherealkiyosaki': 421,\n",
              " 'preparations': 422,\n",
              " '–june': 423,\n",
              " 'money-': 424,\n",
              " 'beck_coulter': 425,\n",
              " 'mumbles': 426,\n",
              " 'lizzy': 427,\n",
              " 'deception': 428,\n",
              " 'pocketbook': 429,\n",
              " 'thomasvanhoutte': 430,\n",
              " 'order': 431,\n",
              " 'trumpdorallink': 432,\n",
              " 'scalelink': 433,\n",
              " 'slater': 434,\n",
              " 'mitzvah': 435,\n",
              " 'specialkmb': 436,\n",
              " 'clayaiken': 437,\n",
              " 'foundation': 438,\n",
              " 'anthonyweiner': 439,\n",
              " 'posers': 440,\n",
              " 'arsenmissyirvin': 441,\n",
              " 'tweetu': 442,\n",
              " 'he': 443,\n",
              " 'mediocrity': 444,\n",
              " 't.link': 445,\n",
              " 'deere': 446,\n",
              " 'reckless': 447,\n",
              " 'thesmartguy': 448,\n",
              " 'eastonelliott': 449,\n",
              " 'unchallenged': 450,\n",
              " 'scholar': 451,\n",
              " 'fathers': 452,\n",
              " 'keithwhittingt': 453,\n",
              " 'the_iron_lad': 454,\n",
              " 'lanka': 455,\n",
              " 'dnt': 456,\n",
              " 'invited': 457,\n",
              " 'bkfviking': 458,\n",
              " 'week': 459,\n",
              " 'cnnopinion': 460,\n",
              " 'positivey': 461,\n",
              " 'notamuni': 462,\n",
              " 'mattyms': 463,\n",
              " 'uttered': 464,\n",
              " 'you—beats': 465,\n",
              " 'valley': 466,\n",
              " 'dannbushh': 467,\n",
              " 'dutchess': 468,\n",
              " 'amenities': 469,\n",
              " 'votes/hundreds': 470,\n",
              " 'requirements': 471,\n",
              " 'underestimate': 472,\n",
              " 'pepsidaisy': 473,\n",
              " 'involvedlink': 474,\n",
              " 'pencey_mouth': 475,\n",
              " 'deqwik': 476,\n",
              " 'westerhout': 477,\n",
              " 'farm_mom': 478,\n",
              " 'ahm': 479,\n",
              " 'changetheworldinwords': 480,\n",
              " 'amit_': 481,\n",
              " 'well': 482,\n",
              " 'if/when': 483,\n",
              " 'kellyannepolls': 484,\n",
              " 'goals': 485,\n",
              " 'officialslink': 486,\n",
              " 'teh_jaybee': 487,\n",
              " 'wld': 488,\n",
              " 'field': 489,\n",
              " 'chosen': 490,\n",
              " 'maybach_zach': 491,\n",
              " 'right-minded': 492,\n",
              " 'bragg_tina': 493,\n",
              " 'spice': 494,\n",
              " 'thedalyplanet': 495,\n",
              " 'useless': 496,\n",
              " 'forth': 497,\n",
              " 'meyers': 498,\n",
              " 'vdare': 499,\n",
              " 'b.link': 500,\n",
              " 'burr': 501,\n",
              " 'youngdemstrump': 502,\n",
              " 'inactivity': 503,\n",
              " 'speaking-but': 504,\n",
              " 'dominating': 505,\n",
              " 'adiosamerica': 506,\n",
              " 'kimberley': 507,\n",
              " 'secnielsen': 508,\n",
              " 'wendypeters': 509,\n",
              " 'awesone': 510,\n",
              " 'craziness': 511,\n",
              " 'platter': 512,\n",
              " 'zukerisaloser': 513,\n",
              " 'jrnicholas': 514,\n",
              " 'maryhukill': 515,\n",
              " 'mrewanmurray': 516,\n",
              " 'o.k': 517,\n",
              " 'personnel': 518,\n",
              " 'farber': 519,\n",
              " 'noon': 520,\n",
              " 'panam': 521,\n",
              " 'german': 522,\n",
              " 'lovebn': 523,\n",
              " '.never': 524,\n",
              " \"'something\": 525,\n",
              " 'derail.': 526,\n",
              " 'thick': 527,\n",
              " 'paytoplay': 528,\n",
              " 'avik': 529,\n",
              " 'cruze': 530,\n",
              " 'trumpcasinos': 531,\n",
              " 'getter': 532,\n",
              " 'mack': 533,\n",
              " \"won't-thanks\": 534,\n",
              " 'seanhannity': 535,\n",
              " 'rebuilding': 536,\n",
              " 'pre-order': 537,\n",
              " 'rlc': 538,\n",
              " 'quick-thinking': 539,\n",
              " 'govtstheproblem': 540,\n",
              " 'election-losing': 541,\n",
              " 'zoom': 542,\n",
              " 'farrell': 543,\n",
              " 'long-shot': 544,\n",
              " 'ecsullie': 545,\n",
              " 'coalescing': 546,\n",
              " 'destination': 547,\n",
              " 'trdny': 548,\n",
              " 'cover-ups': 549,\n",
              " 'doemocrat': 550,\n",
              " 'princeetornam': 551,\n",
              " 'legendisland': 552,\n",
              " 'larry': 553,\n",
              " 'pieces': 554,\n",
              " 'saved': 555,\n",
              " 'dealers': 556,\n",
              " 'positions': 557,\n",
              " 'mrscottmurray': 558,\n",
              " 'involved': 559,\n",
              " 'joneasley': 560,\n",
              " 'daca.': 561,\n",
              " 'watched': 562,\n",
              " 'chuckwoolery': 563,\n",
              " 'recession.': 564,\n",
              " 'canadian': 565,\n",
              " 'अमरीका': 566,\n",
              " 'nathanfuchs': 567,\n",
              " 'jonygitar': 568,\n",
              " 'unconstitutional': 569,\n",
              " 'mileycyrus-': 570,\n",
              " 'last': 571,\n",
              " 'jdrago': 572,\n",
              " 'neilleon_': 573,\n",
              " 'slightly': 574,\n",
              " 'rupert': 575,\n",
              " 'ronpaul': 576,\n",
              " 'jessicarnewman': 577,\n",
              " 'qoute': 578,\n",
              " 'fowho': 579,\n",
              " 'pummeled': 580,\n",
              " 'measured': 581,\n",
              " 'buckybirt': 582,\n",
              " 'nycjennie': 583,\n",
              " 'phyllis': 584,\n",
              " 'kjclt': 585,\n",
              " 'partnership.': 586,\n",
              " 'kurt': 587,\n",
              " 'fume': 588,\n",
              " 'ahya': 589,\n",
              " 'price': 590,\n",
              " 'abedin': 591,\n",
              " 'president': 592,\n",
              " 'bankers': 593,\n",
              " 'interview-the': 594,\n",
              " 'poetry': 595,\n",
              " 'special': 596,\n",
              " 'pramila': 597,\n",
              " '-giving': 598,\n",
              " 'keksec__org': 599,\n",
              " 'zachadkinsz': 600,\n",
              " 'premiers': 601,\n",
              " 'imaginative': 602,\n",
              " 'ibusinesslounge': 603,\n",
              " 'originalholden': 604,\n",
              " 'race.': 605,\n",
              " 'grt': 606,\n",
              " 'tomemmer': 607,\n",
              " 'stole': 608,\n",
              " 'hoft': 609,\n",
              " 'gills': 610,\n",
              " 'five-level': 611,\n",
              " 'soleimani': 612,\n",
              " 'hard-fought': 613,\n",
              " 'dominates': 614,\n",
              " 'careslink': 615,\n",
              " 'wackolink': 616,\n",
              " 'skype': 617,\n",
              " 'renhowe': 618,\n",
              " 'ignore': 619,\n",
              " '.very': 620,\n",
              " 'fry': 621,\n",
              " 'dlayphoto': 622,\n",
              " 'georgians': 623,\n",
              " 'smocking': 624,\n",
              " 'newson': 625,\n",
              " 'hugshenderson': 626,\n",
              " 'dummylink': 627,\n",
              " 'moffiq': 628,\n",
              " 'lady_rin_kelly': 629,\n",
              " 'children-link': 630,\n",
              " 'selfishness': 631,\n",
              " 'slides': 632,\n",
              " 'joebiden': 633,\n",
              " 'enough—or': 634,\n",
              " 'sunglasspros': 635,\n",
              " 'mikem': 636,\n",
              " 'burgess': 637,\n",
              " 'khalidnasser': 638,\n",
              " 'enablers': 639,\n",
              " 'crushing': 640,\n",
              " 'liangelo': 641,\n",
              " 'cuba': 642,\n",
              " 'respondlink': 643,\n",
              " 'scrutiny': 644,\n",
              " 'safe-third': 645,\n",
              " 'djaycameron': 646,\n",
              " 'bestshow': 647,\n",
              " 'parenthood': 648,\n",
              " 'allocating': 649,\n",
              " 'damiendzanic': 650,\n",
              " 'listened': 651,\n",
              " 'schiff': 652,\n",
              " 'ceased': 653,\n",
              " 'beaches': 654,\n",
              " 'caucusgoers': 655,\n",
              " 'aired': 656,\n",
              " 'annoy': 657,\n",
              " 'fullybop': 658,\n",
              " 'suppofrom': 659,\n",
              " 'solidly': 660,\n",
              " 'insidegym': 661,\n",
              " 'fotonight': 662,\n",
              " 'cybergolfnews': 663,\n",
              " 'tahanna': 664,\n",
              " 'legalized': 665,\n",
              " 'roaring': 666,\n",
              " 'thinks': 667,\n",
              " 'upping': 668,\n",
              " 'ed': 669,\n",
              " 'widodo': 670,\n",
              " 'निष्ठावान': 671,\n",
              " 'ellison': 672,\n",
              " 'queenofspice': 673,\n",
              " 'maher': 674,\n",
              " 'nastialiukin': 675,\n",
              " '-with': 676,\n",
              " 'dhs_wolf': 677,\n",
              " 'universifize': 678,\n",
              " 'tomspiglanin': 679,\n",
              " 'mcgarvey': 680,\n",
              " 'brian—are': 681,\n",
              " 'marianoqcasilum': 682,\n",
              " 'mette': 683,\n",
              " 'hcq': 684,\n",
              " 'keeli_munkres': 685,\n",
              " 'mikedgarrison': 686,\n",
              " 'unattractive': 687,\n",
              " 'fopolllink': 688,\n",
              " 'uum': 689,\n",
              " 'shattering': 690,\n",
              " 'chooses': 691,\n",
              " 'incurred': 692,\n",
              " 'hrguyserchia': 693,\n",
              " 'conductor': 694,\n",
              " 'hiring': 695,\n",
              " 'declaring': 696,\n",
              " 'announce': 697,\n",
              " 'appealing': 698,\n",
              " 'firebrittmchenry': 699,\n",
              " 'deanhellerlink': 700,\n",
              " 'mayorluigiboria': 701,\n",
              " 'whack-job': 702,\n",
              " 'finger_slinger': 703,\n",
              " 'eriksson': 704,\n",
              " 'sugar': 705,\n",
              " 'warplanes': 706,\n",
              " 'moyers': 707,\n",
              " 'samahdanash': 708,\n",
              " 'jackevansward': 709,\n",
              " 'kotcha': 710,\n",
              " 'rudygiuliani': 711,\n",
              " 'process.': 712,\n",
              " 'roniseale': 713,\n",
              " 'valentine': 714,\n",
              " 'est.': 715,\n",
              " 'origins': 716,\n",
              " 'fwproud': 717,\n",
              " 'rebuke': 718,\n",
              " 'pinnacle': 719,\n",
              " 'thameena': 720,\n",
              " 'attends': 721,\n",
              " 'dreamt': 722,\n",
              " 'rationally': 723,\n",
              " 'trial-but': 724,\n",
              " 'postponed': 725,\n",
              " 'therickwilson': 726,\n",
              " 'thejbish': 727,\n",
              " 'video-': 728,\n",
              " 'monkiekaty': 729,\n",
              " 'tsa': 730,\n",
              " 'derekamartinez': 731,\n",
              " 'layin': 732,\n",
              " 'stoked': 733,\n",
              " 'story-want': 734,\n",
              " 'alwaystrump': 735,\n",
              " 'wouldnt': 736,\n",
              " 'securitylink': 737,\n",
              " 'sounded': 738,\n",
              " 'fastlink': 739,\n",
              " 'borntobegop': 740,\n",
              " 'defensive': 741,\n",
              " 'reps—increase': 742,\n",
              " 'burned': 743,\n",
              " 'ahmedmallick': 744,\n",
              " 'counterpart': 745,\n",
              " 'committed': 746,\n",
              " 'endinglink': 747,\n",
              " 'joaquin': 748,\n",
              " 'farther': 749,\n",
              " 'bighead': 750,\n",
              " 'catastrophes': 751,\n",
              " 'globalist': 752,\n",
              " 'countrieslink': 753,\n",
              " 'relationship-but': 754,\n",
              " 'revive': 755,\n",
              " 'prosecutors': 756,\n",
              " 'scam': 757,\n",
              " 'smaone': 758,\n",
              " 'gerard—wonderful': 759,\n",
              " 'am': 760,\n",
              " 'phoenihe': 761,\n",
              " 'mnrosrnr': 762,\n",
              " 'joseconesa': 763,\n",
              " 'arizona-': 764,\n",
              " \"'dummy\": 765,\n",
              " 'jiminhofe': 766,\n",
              " 'blackmail': 767,\n",
              " 'gleisissantos': 768,\n",
              " 'progressive': 769,\n",
              " 'good.': 770,\n",
              " 'pammygb': 771,\n",
              " \"'privately\": 772,\n",
              " 'bowl': 773,\n",
              " 'even': 774,\n",
              " 'jentezen': 775,\n",
              " 'hoashe': 776,\n",
              " 'shaunsettle': 777,\n",
              " 'outlast': 778,\n",
              " 'majestic': 779,\n",
              " '.security': 780,\n",
              " 'coollink': 781,\n",
              " 'rejecting': 782,\n",
              " 'sandusky': 783,\n",
              " 'truthtrump': 784,\n",
              " 'bgoldsmith': 785,\n",
              " 'ohhh': 786,\n",
              " 'indie': 787,\n",
              " 'b-day': 788,\n",
              " 'information.': 789,\n",
              " 'microger': 790,\n",
              " 'mate': 791,\n",
              " 'mattblunt': 792,\n",
              " 'ghostdancer_': 793,\n",
              " 'angrycrank': 794,\n",
              " 'wonderful.': 795,\n",
              " 'foerderer': 796,\n",
              " 'business.': 797,\n",
              " 'yourefired': 798,\n",
              " \"payroll'link\": 799,\n",
              " 'despises': 800,\n",
              " 'georgeptransue': 801,\n",
              " 'reputations': 802,\n",
              " 'self-fund': 803,\n",
              " 'plz': 804,\n",
              " 'thereddotguy': 805,\n",
              " 'fallout': 806,\n",
              " 'mccain-': 807,\n",
              " 'fulcher': 808,\n",
              " 'americanlink': 809,\n",
              " 'lisamurkowski': 810,\n",
              " 'abysmal.': 811,\n",
              " 'songs': 812,\n",
              " 'spirtualhappy': 813,\n",
              " 't.v.': 814,\n",
              " \"'signs\": 815,\n",
              " 'whatamericaneeds': 816,\n",
              " 'morales': 817,\n",
              " 'irony': 818,\n",
              " 'overdose': 819,\n",
              " 'sundaylink': 820,\n",
              " 'steffbrown': 821,\n",
              " 'scprovingground': 822,\n",
              " 'playbook-': 823,\n",
              " 'softly': 824,\n",
              " 'build-up': 825,\n",
              " 'avenue': 826,\n",
              " 'drgoodspine': 827,\n",
              " \"that'link\": 828,\n",
              " 'sells': 829,\n",
              " 'terribly': 830,\n",
              " 'trublumajority': 831,\n",
              " 'defrauding': 832,\n",
              " 'crosses': 833,\n",
              " 'craigsj': 834,\n",
              " 'representation': 835,\n",
              " 'ukcarioca': 836,\n",
              " 'wtcommunities': 837,\n",
              " 'larryschweikart': 838,\n",
              " 'double': 839,\n",
              " 'dts': 840,\n",
              " 'leadrightlink': 841,\n",
              " 'sensational': 842,\n",
              " 'lecirquenyc': 843,\n",
              " 'confirmgorsuch': 844,\n",
              " 'mikeneedham': 845,\n",
              " 'utilize': 846,\n",
              " 'vibow': 847,\n",
              " 'biker': 848,\n",
              " 'everywhere': 849,\n",
              " 'fitch': 850,\n",
              " 'cursing': 851,\n",
              " 'jonyeong': 852,\n",
              " 'isler': 853,\n",
              " 'bennett': 854,\n",
              " 'grandmiapens': 855,\n",
              " 'vine': 856,\n",
              " 'levell': 857,\n",
              " 'servants': 858,\n",
              " 'minneapolis': 859,\n",
              " 'untalented': 860,\n",
              " 'charade': 861,\n",
              " 'howardstern': 862,\n",
              " 'authorization': 863,\n",
              " 'manninglink': 864,\n",
              " 'alikilmartin': 865,\n",
              " 'mullahs': 866,\n",
              " 'derektheeight': 867,\n",
              " 'candidates.': 868,\n",
              " 'faizermiami': 869,\n",
              " 'suburbs': 870,\n",
              " 'peterson': 871,\n",
              " 'montoursville': 872,\n",
              " 'icallbullsh_t': 873,\n",
              " 'osprey': 874,\n",
              " 'woodrowwinters': 875,\n",
              " 'bmw_eforever': 876,\n",
              " 'camastoras': 877,\n",
              " 'book…': 878,\n",
              " 'underperforming': 879,\n",
              " 'kanye': 880,\n",
              " '_jacksmith': 881,\n",
              " 'weber': 882,\n",
              " 'emilymiller': 883,\n",
              " 'soloman': 884,\n",
              " 'banana': 885,\n",
              " 'boardrooms—can': 886,\n",
              " 'understands': 887,\n",
              " 'tell-all': 888,\n",
              " 'thasugarmaker': 889,\n",
              " 'kelliheathman': 890,\n",
              " 'pension': 891,\n",
              " 'hershey': 892,\n",
              " 'resign-he': 893,\n",
              " 'saying': 894,\n",
              " 'mystery': 895,\n",
              " 'damon': 896,\n",
              " 'congresswomen': 897,\n",
              " 'moan': 898,\n",
              " 'leftwing': 899,\n",
              " 'wyatt': 900,\n",
              " 'louisianagov': 901,\n",
              " 'اهمیتی': 902,\n",
              " 'sides': 903,\n",
              " 'asa_patriot': 904,\n",
              " 'portrait': 905,\n",
              " 'd.j.t': 906,\n",
              " 'postpone': 907,\n",
              " 'scienceinvestme': 908,\n",
              " 'cloud': 909,\n",
              " 'bowing': 910,\n",
              " 'doomed': 911,\n",
              " 'nlypablo': 912,\n",
              " 'furiousblaze': 913,\n",
              " 'sundropaholic': 914,\n",
              " 'tapestry': 915,\n",
              " 'jim': 916,\n",
              " 'run-around': 917,\n",
              " 'co-author': 918,\n",
              " 'jasondufner': 919,\n",
              " 'is': 920,\n",
              " 'thomasbrennan': 921,\n",
              " 'east-west': 922,\n",
              " 'keys': 923,\n",
              " 'newnonny': 924,\n",
              " 'finkelstein': 925,\n",
              " 'bathrooms': 926,\n",
              " 'emeka': 927,\n",
              " 'fishbowldc': 928,\n",
              " 'betrays': 929,\n",
              " 'can_u_see_': 930,\n",
              " 'wny': 931,\n",
              " 'reined': 932,\n",
              " 'timdevaney': 933,\n",
              " 'dkc': 934,\n",
              " 'architectural': 935,\n",
              " 'sirius': 936,\n",
              " 'dhsgov': 937,\n",
              " 'reneei': 938,\n",
              " 'canadaand': 939,\n",
              " 're-building': 940,\n",
              " 'skit': 941,\n",
              " 'ktsarita': 942,\n",
              " 'happy': 943,\n",
              " 'rosenavy': 944,\n",
              " 'samstwitch': 945,\n",
              " 'morninglink': 946,\n",
              " 'sort': 947,\n",
              " 'arthurkade': 948,\n",
              " 'mymantrump': 949,\n",
              " 'sweetheart': 950,\n",
              " 'batchelorshow': 951,\n",
              " 'taestates': 952,\n",
              " 'jackiehearns': 953,\n",
              " 'comments': 954,\n",
              " 'fl-help-': 955,\n",
              " 'swamped': 956,\n",
              " 'shed': 957,\n",
              " 's.s.': 958,\n",
              " 'nbcnews': 959,\n",
              " 'choir': 960,\n",
              " 'ryan': 961,\n",
              " 'adjunct': 962,\n",
              " 'congratulations': 963,\n",
              " 'gerriwillisfbn': 964,\n",
              " 'newton': 965,\n",
              " 'trottofficial': 966,\n",
              " 'dicky': 967,\n",
              " 'prayingforryan': 968,\n",
              " 'belikehannah': 969,\n",
              " 'seas': 970,\n",
              " 'lauri': 971,\n",
              " 'jerryyanks': 972,\n",
              " 'ken': 973,\n",
              " 'intend': 974,\n",
              " 'disability': 975,\n",
              " 'mcllroyrory': 976,\n",
              " 'hudsonmod': 977,\n",
              " 'trophy—for': 978,\n",
              " 'defeatism': 979,\n",
              " 'bluestarfan': 980,\n",
              " 'luvoinc': 981,\n",
              " 'covid-': 982,\n",
              " 'newspaperlink': 983,\n",
              " 'wan': 984,\n",
              " 'bohai': 985,\n",
              " 'ryanjorde': 986,\n",
              " 'sethmacfarlane': 987,\n",
              " 'ok.': 988,\n",
              " 'rebellion': 989,\n",
              " 'husband': 990,\n",
              " 'transparent': 991,\n",
              " 'headlines': 992,\n",
              " 'soul_of_twit': 993,\n",
              " 'property—and': 994,\n",
              " 'signed': 995,\n",
              " 'villa': 996,\n",
              " 'longtallte': 997,\n",
              " 'icegov': 998,\n",
              " 'shepherdgarrett': 999,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPvUebuqx7Zv"
      },
      "source": [
        "# Problem 1: N-gram Language Model\n",
        "In this problem, you will be building a couple n-gram language modeling and see how well just taking pure frequency counts and building a conditional probability distrbution will work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcidjoN9yfDn"
      },
      "source": [
        "## Task 1: A 2-gram (Bigram) Model\n",
        "Recall that our goal in building a language model is to represent the conditional probability $P(w_i | w_{i-1})$ for pairs of words $w_i$ and $w_j$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udSAfh76ypCS"
      },
      "source": [
        "### Part A: Frequencies\n",
        "Go through the encoded Trump tweets and calculate the frequencies of all words as well as all pair of words that appear next to each other in the corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qbHlzNfazdB1"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def make_grams(tweets, n=2):\n",
        "    grams = []\n",
        "    for tweet in tweets:\n",
        "        # Add bigrams to the list\n",
        "        for i in range(len(tweet) - n + 1):\n",
        "            grams.append(tuple(tweet[i:i + n]))\n",
        "    return grams\n",
        "def calculate_counts(grams):\n",
        "    # Counting frequencies of each gram\n",
        "    counts = defaultdict(int)\n",
        "    for gram in grams:\n",
        "        counts[gram] += 1\n",
        "    return counts\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigrams = make_grams(train_data, n=2)\n",
        "\n",
        "# Calculate bigram frequencies\n",
        "bigram_counts = calculate_counts(bigrams)\n",
        "\n",
        "# Display the calculated counts\n",
        "print(bigram_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLD0rAy_oMKN",
        "outputId": "2be864ee-cf33-4352-d0a8-701c7f7a364f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Y5ddjeM31Qlu"
      },
      "outputs": [],
      "source": [
        "grams = make_grams(train_data)\n",
        "counts = calculate_counts(grams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaNdT9dO1GNM"
      },
      "source": [
        "### Part B: Probabilities\n",
        "Now from the counts above we will calculate an associated conditional probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Xn_rcmtn1SYE"
      },
      "outputs": [],
      "source": [
        "# probs = defaultdict(dict)\n",
        "# #for gram, count in counts.items():\n",
        "#   #TODO: Do something...\n",
        "# x = 0\n",
        "\n",
        "#   # Calculate probabilities for each bigram\n",
        "# for bigram, count in bigram_counts.items():\n",
        "#     # Extract the first and second word from the bigram\n",
        "#     word1, word2 = bigram\n",
        "#     x += 1\n",
        "#     # Calculate the conditional probability P(word2 | word1)\n",
        "#     probability = count / sum(bigram_counts[prev_word, current_word] for prev_word, current_word in bigram_counts if prev_word == word1)\n",
        "\n",
        "#     # Store the probability in the dictionary\n",
        "#     probs[word1][word2] = probability\n",
        "\n",
        "# # Display the calculated probabilities\n",
        "# print(probs)\n",
        "\n",
        "unigram_counts = defaultdict(int)\n",
        "for (word1, word2), count in bigram_counts.items():\n",
        "    unigram_counts[word1] += count\n",
        "\n",
        "# Now, calculate the conditional probabilities\n",
        "probs = defaultdict(dict)\n",
        "for (word1, word2), count in bigram_counts.items():\n",
        "    probability = count / unigram_counts[word1]\n",
        "    probs[word1][word2] = probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFK8qmFg1o8V"
      },
      "source": [
        "### Part C: Make an Bigram Generator Function\n",
        "Now that we have our probability distrbution, let's make a generator function so that we can generate random Trump tweets using our bigram language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qe3lugnI1dRS"
      },
      "outputs": [],
      "source": [
        "# def get_next_word(next_word_probs):\n",
        "#   #TODO: Write a function to get the next word based on the previous words conditional probabilities.\n",
        "#   return something\n",
        "\n",
        "# def generate(start_text='', n=10, probs=probs, vocabulary=vocabulary, vocabulary_inverse=vocabulary_inverse):\n",
        "#   # Helper code to create the start text.\n",
        "#   start_text = ['<s>'] + nltk.word_tokenize(preprocess(start_text))\n",
        "\n",
        "#   #TODO: Encode and generate a trump tweet.\n",
        "#   return something\n",
        "\n",
        "import random\n",
        "\n",
        "def get_next_word(next_word_probs):\n",
        "    # Convert the probability distribution into a list of (word, prob) tuples\n",
        "    words, probs = zip(*next_word_probs.items())\n",
        "    # Randomly choose a word based on the provided probabilities\n",
        "    next_word = random.choices(words, weights=probs, k=1)[0]\n",
        "    return next_word\n",
        "\n",
        "def generate(start_text='', n=10, probs=probs, vocabulary=vocabulary, vocabulary_inverse=vocabulary_inverse):\n",
        "    # Tokenize and preprocess the start text\n",
        "    start_text_tokens = ['<s>'] + nltk.word_tokenize(preprocess(start_text))\n",
        "\n",
        "    # Initialize the generated text with the start text\n",
        "    generated_text = start_text_tokens\n",
        "\n",
        "    # Generate n words\n",
        "    for _ in range(n):\n",
        "        # Get the last word in the current text\n",
        "        last_word = generated_text[-1]\n",
        "        # Get the encoded version of the last word\n",
        "        last_word_encoded = vocabulary.get(last_word, None)\n",
        "\n",
        "        # If the last word is not in the vocabulary or there's no next word, break\n",
        "        if last_word_encoded is None or last_word_encoded not in probs:\n",
        "            break\n",
        "\n",
        "        # Get the next word probabilities\n",
        "        next_word_probs = probs[last_word_encoded]\n",
        "\n",
        "        # Get the next word and decode it\n",
        "        next_word_encoded = get_next_word(next_word_probs)\n",
        "        next_word = vocabulary_inverse[next_word_encoded]\n",
        "\n",
        "        # Append the next word to the generated text\n",
        "        generated_text.append(next_word)\n",
        "\n",
        "        # If the next word is the end token, break\n",
        "        if next_word == '</s>':\n",
        "            break\n",
        "\n",
        "    # Join the words to form the final text\n",
        "    return ' '.join(generated_text[1:])  # Exclude the start token from the final text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_4ssQiWK3VnD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "03f32a35-d36e-44d5-cba7-13aa6cd2086f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'i can make the same with wolfblitzer from highways to be really looking'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "generate('I can make')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WC5ZUiZD1-ew"
      },
      "source": [
        "## **(Bonus)** Task 2: Make a Trigram Model\n",
        "Using your code from Parts A-C, see if you can build a trigram (3-gram) model that might make tweets a little more logical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "aY_vnGqB2FxG"
      },
      "outputs": [],
      "source": [
        "#TODO: Your code goes here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cq3fmX8K_l_N"
      },
      "source": [
        "# Problem 2: Using a Transformer\n",
        "In this section we are going to leverage a pretrained transformer, i.e., [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), built by the [Hugging Faces Team](https://huggingface.co/gpt2?text=A+long+time+ago%2C). We will also be using their tokenizers because they have been optimized for the language generation task. You should be aware that behind the scenes, their model is using [PyTorch](https://pytorch.org/), the deep learning library built by Facebook, and is quickly becoming more popular than our beloved Tensorflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJNX1muh_7IA"
      },
      "source": [
        "## Task 1: Install and load the GPT2 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "HAUkPd1lAFHh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3daad87a-af10-49a1-d6cb-b7e0fc5e1389"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "# Run the following code to install the HuggingFace's transformer python package.\n",
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8vf9LGqwfTmd"
      },
      "outputs": [],
      "source": [
        "# TODO: Follow the link above and load the GPT2 model as well as the tokenizer.\n",
        "# NOTE: Replace GPT2Model with GPT2LMHeadModel (this is the model for finetuning use cases.)\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voxq5kGXs3fv"
      },
      "source": [
        "## Task 2: Formatting our Training Data\n",
        "Upcoming we will be fine-tunning this GPT-2 model on our Trump Tweets, but in order to leverage some of the utility classes built by HuggingFaces, we want to take our preprocessed Trump tweets and place them in a flat text file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "3ZEdK90df8WG"
      },
      "outputs": [],
      "source": [
        "#TODO: Take our preprocessed trump tweets and write them to a text file\n",
        "with open('trumpdata.txt', 'w', encoding='utf-8') as text_file:\n",
        "    for tweet in real_donald_trump_df['content']:\n",
        "        # Write each tweet followed by a newline character\n",
        "        text_file.write(tweet + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koMncJbEtoyg"
      },
      "source": [
        "## Task 4: Fine-tuning the Model\n",
        "In the following sections, we will complete a couple functions that will allow us to fine-tune the GPT-2 model to our Trump Tweets. I am going to give you a couple of these helper functions, but leave you to write parts of the training function. See the following documentation from Hugging Faces to see the attributes for the [Trainer Class](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "kxJQdEcx3r4g"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "\n",
        "def load_dataset(file_path, tokenizer, block_size = 128):\n",
        "    # Will load and tokenize the data\n",
        "    dataset = TextDataset(\n",
        "        tokenizer = tokenizer,\n",
        "        file_path = file_path,\n",
        "        block_size = block_size,\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "def load_data_collator(tokenizer, mlm = False):\n",
        "    # Helper Function\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=mlm,\n",
        "    )\n",
        "    return data_collator\n",
        "\n",
        "def train(\n",
        "    train_file_path,\n",
        "    model,\n",
        "    tokenizer,\n",
        "    output_dir,\n",
        "    overwrite_output_dir,\n",
        "    num_train_epochs,\n",
        "    save_steps\n",
        "    ):\n",
        "    # Load the dataset\n",
        "    dataset = load_dataset(train_file_path, tokenizer)\n",
        "\n",
        "    # Load the data collator\n",
        "    data_collator = load_data_collator(tokenizer, mlm=False)\n",
        "\n",
        "    # Initialize TrainingArguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        overwrite_output_dir=overwrite_output_dir,\n",
        "        per_device_train_batch_size=8,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        save_steps=save_steps,\n",
        "    )\n",
        "\n",
        "    # Initialize the Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=dataset,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the model and tokenizer\n",
        "    trainer.save_model(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "3drTxu5p39K1"
      },
      "outputs": [],
      "source": [
        "#TODO: Set necessary parameters, here are some defaults.\n",
        "train_file_path = \"/content/trumpdata.txt\"\n",
        "output_dir = '/content/result'\n",
        "overwrite_output_dir = False\n",
        "num_train_epochs = 1\n",
        "save_steps = 500"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S76YKbgYLd9W",
        "outputId": "a3d0c132-601c-415d-f161-c3cd855e93d1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.24.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "bLFjEBY44FoD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "827878e0-880b-48d2-8e7b-c979b91e0716"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1274' max='1274' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1274/1274 06:36, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>3.852200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>3.577700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#TODO: Use your train function to train the model. It takes about 30 minutes to train in colab.\n",
        "train(\n",
        "    train_file_path=train_file_path,\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=overwrite_output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    save_steps=save_steps\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6MbNciWv0HT"
      },
      "source": [
        "## Task 5: Creating a Tweet Generator\n",
        "Now that we have our trained model, it's time to generate some Tweets. Since we should have saved our model and tokenizer to an output directory, I've already made some helper functions to load those in. We will focus on our *generate_text* function. The function will take as input some start text like \"I am\" or \"My country is\", etc., as well as a max_length parameter which tells the model how much text to generate. Let's Go!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "GOXahdOC4I9q"
      },
      "outputs": [],
      "source": [
        "def load_model(model_path):\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "    return model\n",
        "\n",
        "def load_tokenizer(tokenizer_path):\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
        "    return tokenizer\n",
        "\n",
        "def generate_text(sequence, max_length):\n",
        "    # Load in the finetuned model and tokenizer\n",
        "    model = load_model('/content/result')  # Assuming this is the model path\n",
        "    tokenizer = load_tokenizer('/content/result')  # Assuming this is the tokenizer path\n",
        "\n",
        "    # Encode our passed sequence\n",
        "    ids = tokenizer.encode(sequence, return_tensors='pt')\n",
        "\n",
        "    # Use the generate method in our model class to generate output tokens\n",
        "    final_outputs = model.generate(\n",
        "        input_ids=ids,\n",
        "        max_length=max_length,\n",
        "        do_sample=True,\n",
        "        pad_token_id=model.config.eos_token_id,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "    )\n",
        "\n",
        "    # Function to print and decode output\n",
        "    print(tokenizer.decode(final_outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "KC8O6Fnh4X-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e63a7b96-439f-47bb-a909-e00fde3334e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I can make my own mistakes, and sometimes these are mistakes, but my life is much better with a smile than the u.s.a.\n"
          ]
        }
      ],
      "source": [
        "#TODO: Now generate some text!\n",
        "generate_text('I can make', 30)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9nPyDfCjOsYg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}